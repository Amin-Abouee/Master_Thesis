%!TEX root = main.tex
\chapter{Introduction}\label{chapter:introduction}
Mankind always have been pursued to find their location and position in world correctly. By advancement of science and using of new electronic devices, this requirement have been resolved. Generally for locating, we need a reference point as origin or (0,0,0) to express the location of an object in every moment relative to this reference. This procedure will be more complicated when the desire object has pace. To estimate the position of moving object in each time unit, The difference of displacement, is calculated and updated.\\
Augmented Reality (AR) is a new technology that aims to generate a composite view for users. This view is a combination of the real view that user can see it and a virtual view such as graphics, sounds or animations which generates by computer. To augment the additional information to the real world, the geometry relation between the world and camera is necessary. These geometry relations that describes the position of camera relative to reference point in every moment is called tracking.\\
Tracking an object is a fundamental part of Augmented Reality (AR). Tracking means finding the location of an object or camera when they have movement in a sequence of frames relative to a reference point. Based on the AR application and degree of freedom of the object and the camera, there are two main tracking approaches:

\begin{itemize}
\item 2D Tracking: Estimate a 2D transformation which describes the 3D displacement of image projection of objects or a part of objects.
\item 3D Tracking: Identify the camera rotation and translation relative to the scene. It contains of 3 degrees of freedom for rotation and 3 degree for translation.
\end{itemize}

Due to the target applications and existence of so many mathematic approaches for solving the 3D tracking using a single camera, research in this field is substantially huge. marker-based and marker-less natural features-based techniques, are two methods to find out the position of camera or 3D objects tracking.\\
In this master's thesis, we developed a new and novel approach rely on the marker-less natural features-based to track the moving of a monocular camera. We assume the whole world is static except of camera. For the first phase of natural feature 3D tracking, feature points matching between each two sequence frames is critical and essential. An innovative method is developed, called robust feature matching, that extremely decrease the number of mismatching feature points. The other novelty of this master thesis that is unique and implemented for the first time is grouping the input images into the small size groups called bundles. Using of bundles (usually 5 frames) instead of a single frame increase significantly the accuracy of camera pose estimation. The result of this master's thesis is comparable with the state-of-the-art approaches in 3D tracking.\\

\section{Related Work}
\subsection{PTAM}
Georg Klein and David Murry \cite{klein2007parallel} proposed a method of estimating the pose (rotation and translation) of a hand-held camera without any prior knowledge about an small AR environment. The idea was adapted from SLAM algorithms in robotic or SFM in computer vision with a novelty in implementation. Both SLAM and SFM usually can be divided into two major tasks:
\begin{itemize}
\item Tracking: Track the motion of hand-held camera robustly.
\item Mapping: Produce a 3D feature points from environment that are seen from camera. This 3D World is used for increase the accuracy of tracking task.
\end{itemize}
The key difference of PTAM algorithm compare to the simple SLAM and SFM is that uses of two parallel processing thread for executing the tracking and mapping tasks. This allows them to do this operation in the real-time.\\

\subsection{Ubitrack Framework}
Ubitrack Framework is an open source framework for Augmented Reality. It was developed by Fachgebiet Augmented Reality chair (FAR) of the computer science faculty at Technical University of Munich.

\chapter{Robust Feature Matching}\label{chapter:Robust Feature Matching}
As it was mentioned in introduction, There are two major methods for 3D tracking. Marker-based 3D tracking system, requires the marker and knowledge about the environment to estimate the pose of camera. Some times due to some restriction about the environment (e.g. outdoor environment) and tracking situation, this task is hard or is impossible. Hence, it is better to rely on the features that naturally are extracted from images. It is notable in this approach, more meta informations such as a model of scene object, a set of planer parts and some information about the camera is necessary. To make this procedure independent of 3D scene models, Vincent lepetit and Pascal Fua \cite{lepetit2005monocular} introduced a method that can explorer simultaneously both camera trajectory (tracking) and 3D world map of environment (mapping). \\
For the natural-based 3D Tracking, The whole literature in this subject can be organized into two big families regarding to the properties of image's features that we want to use.
\begin{itemize}
\item Edge-Based methods investigate on the areas with the highest value of gradient that represents edges. These methods try to make an accurate match between the projection of 3D edges in the real world and these gradient points. RAPID (SITE), Explicit Edge Extraction and Direct Optimization on Gradient are some algorithms in this area. 
% http://en.wikipedia.org/wiki/Feature_detection_(computer_vision) for example.
\item The other methods investigate on the information that can be extracted directly from the pixels of image. It can be derived from optical flow, template matching and interest points techniques.
\end{itemize}

\section{Interest Point Method (Corner-Based)}
Despite of Edge-Based methods that need a overall view of the whole image to find the high gradient points and contours,(global view), The interest point-based methods just use of localized features. The difference key between these two approaches is, an interest point is a intersection of two edges that represents a point in which the direction of these two edges changed. Consequently, the gradient of this point have a huge amount in both direction and this property help to can be detected easily.\\
In the case of similarity, both Edge-Based and Interest Point-Based techniques rely on the matching the features among the frames independent of other points. They also can handle the partial occlusions and matching errors problems and are invariant to change of illumination. The advantage of interest point-based methods compared to edge-based methods is that they do not get confused by background clutter. Generally interest points must be uniquely recognizable between all image pixels.\\
After that in this master thesis, instead of interest points, we used of feature points term.\\
Comparing the all pixels of two or more images, one by one is really difficult and high computationally time. instead of this approach, some unique interest points are extracted (feature detection) from images and then coded into a binary string or a float number (feature extraction) and called the feature descriptor.\\

\subsection{Feature Detection}
 The efficient detection of interesting features is a crucial step for many computer vision applications. For detection the features points, attention to some properties of points should be considered as follow: \cite{forstner1986feature}
\begin{enumerate}
  \item The feature points represented by a surrendered patch around the center. This patch should be textured.
  \item Two neighbor feature points, should be different.
  \item Two almost similar feature points (usually neighbor or from same pattern) should be ignored.
  \item The feature selection operation, should be had the same result and performance in all images. It means if a point was selected as a feature point in one section of image, it also should be selected as a feature point in the same section but in different images.
\end{enumerate}
For the first time, Harris and Stephen \cite{harris1988combined} introduced a new method for feature detection regarding to fact that corner feature point represent a variation in the gradient of a point in two direction. they were looked for this variation.\\
Presently, with knowledge expansion in computer vision community, So many techniques were developed that are useful based on the situation of images. Some of the most versatile and popular feature detector are:
\begin{itemize}
\item SIFT \cite{lowe2004distinctive} based of difference of Gaussians (DoG).
\item SURF \cite{bay2006surf} is an instance of 2D Haar wavelet.
\item FAST \cite{rosten2010faster} use of heuristic model for feature point and machine learning techniques.
\end{itemize}
In computer vision community, a feature point or interest point also called keypoint.

\subsection{Feature Extraction}
Once feature points are located, we are interested in describing the image patch with a robust feature vector. The most well-known descriptor was proposed in SIFT \cite{lowe2004distinctive}. A 128-dimensional vector is obtained from a grid of histogram. It was a powerful descriptor and robustness to illumination and change of view. The other approach that is wildly used at moment is SURF \cite{bay2006surf}. Its idea based on local gradient histogram and it has similar performance as SIFT. The point of SURF algorithm is that it can be run more faster with less memory as SIFT algorithm. Both of these approaches describe a keypoint with a long vector of float number in which SIFT with 128-dimensional and SURF with 64 and 128-dimensional.\\
In 2010, Calonder \textit{et al.} in \cite{calonder2010brief} showed that it is possible to decrease the dimensionality a feature descriptor by directly building a short binary descriptor in which each bits are independent. This new algorithm called BRIEF. using of Hamming distance (bitwise XOR) instead of Euclidean distance is an advantage point for binary descriptors. The negative point of BRIEF algorithm was that the result descriptor is not invariant to scale and rotation change. After that both BRISK \cite{leutenegger2011brisk} and FREAK \cite{alahi2012freak} were developed that are state-of-the-art algorithms rely on binary descriptors and also invarient to scale and rotation. BRISK and FREAK are inspired by intensity comparisons around a keypoint and human visual system (especially retina) respectively.

\subsection {KAZE and A-KAZE}
In this master thesis we were looking for a best feature descriptor that should be invariant to brightness, scale, rotation and robustness to blur. Between all feature detectors and extractors that already mentioned here, The KAZE (Japanese word meaning \emph{wind}) and especially A-KAZE (Accelerated-KAZE) feature detector and extractor was selected. Right now it has the best result among all algorithms in this subject. The structure of its descriptor is binary and is added to the new version of OpenCV 3.0 \footnote{\url{http://docs.opencv.org/trunk/modules/features2d/doc/feature_detection_and_description.html}}.
KAZE \cite{alcantarilla2012kaze} feature is a new 2D multi-scale feature detection and description that its algorithm rely on nonlinear scale space. Previous methods such as SIFT or SURF find features in the Gaussian scale space (particular instance of linear diffusion). However, Gaussian blurring does not respect the natural boundaries of objects and smooths in the same degree details and noise when evolving the original image through the scale space.\\
By means of nonlinear diffusion we can detect and describe features in nonlinear scale spaces keeping important image details and removing noise as long as we evolve the image in the scale space. We use variable conductance diffusion which is one of the simplest cases of nonlinear diffusion. The nonlinear scale space is build efficiently by means of Additive Operator Splitting (AOS) schemes, which are stable for any step size and are parallelizable.\\
A-KAZE \cite{alcantarilla2011fast} Features uses a novel mathematical framework called Fast Explicit Diffusion (FED) embedded in a pyramidal framework to speed-up dramatically the nonlinear scale space computation. In addition, we compute a robust Modified-Local Difference Binary (M-LDB) descriptor that exploits gradient information from the nonlinear scale space. A-KAZE obtains comparable results to KAZE in some datasets, while being several orders of magnitude faster.\\
Despite of moderate increase in computational time, especially compare to SURF, but the AKZE feature algorithms is a forward step in performance and accuracy for both detection and description against previous state-of-the-art algorithms such as SURF, SIFT, BRISK and FREAK.
The below graphs are taken form computer-vision-talks weblog \footnote{\url{http://computer-vision-talks.com/articles/2013-03-17-porting-kaze-features-to-opencv/}} and prepared by OpenCV Comparison Tool \footnote{\url{https://github.com/BloodAxe/OpenCV-Features-Comparison}}. It compare the KAZE feature with the other well-known algorithms from several aspects of invariantly such as rotation, scale and brittleness and robustness to blur. As you can see, in all cases, the result of KAZE in much better.

\begin{figure}[H]
\begin{tabular}{cc}
  \includegraphics[width=75mm]{figures/rotation_KAZE} &  \includegraphics[width=75mm]{figures/scale_KAZE} \\
(a) Rotation Test & (b) Scale Invariance \\[6pt]
 \includegraphics[width=75mm]{figures/brightness_KAZE} &  \includegraphics[width=75mm]{figures/blur_KAZE} \\
(c) Brittleness Invariance & (d) Robustness to Blur \\[6pt]
\end{tabular}
\caption{The comparison of KAZE and other well-know feature descriptions}\label{fig:compare_kaze}
\end{figure}

As we mentioned in the definition of A-KAZE, it uses of two new algorithms (FED and M-LDB) and consequently the computation cost for deception and description decrease dramatically. It also extends to support four different type of extracted descriptor [] depend on image properties. In the next graph, A comparison between four top binary descriptors, A-KAZE, KAZE, BRISK, FREAK, was prepared by computer-vision-talks weblog. All algorithms tested for variety of environments and invariantly to scale, brittleness and robustness to blur.
$$(Descriptor_KAZE, Descriptor_KAZE_upright, Descriptor_MLDB and Descriptor_MLDB_upright)$$

\begin{figure}[H]
\begin{tabular}{cc}
  \includegraphics[width=75mm]{figures/brithness_akaze} &  \includegraphics[width=75mm]{figures/scale_akaze} \\
(a) Brittleness Invariance & (b) Scale Invariance \\[6pt]
\multicolumn{2}{c}{\includegraphics[width=75mm]{figures/blur_akaze} }\\
\multicolumn{2}{c}{(e) Robustness to Blur}
\end{tabular}
\caption{The comparison between KAZE and A-KAZE}\label{fig:compare_kaze_and_A-kaze}
\end{figure}

Except of the original authors implementation in \url{https://github.com/pablofdezalc/akaze}, both KAZE and A-KAZE are ported to the new version of OpenCV 3.0 and boosted by OpenCL. In this master thesis, the A-KAZE implementation of OpenCV is used for both feature detection and description. The parameters of A-KAZE optimized based of our requirement and listed as below table:

\begin{table}[H]
  \caption{A-KAZE value parameters in OpenCV 3.0}
  \begin{tabular}{| c | c | c | c | c | c | c |}
      \hline
      descriptor type & descriptor size & descriptor channels & threshold & octaves & sublevels & diffusivity \\ \hline \hline
      Descriptor dump & Full & 3 & 0.001 & 4 & 4 & dump \\ \hline
  \end{tabular}
\end{table}

\section {Feature Point Matching}
For pose estimation, we need to find out a match set of feature points ${m_i}$ and ${m_j}$ which extracted from two successive images with similar view points. The classical technique proposed by Zhan et al. \cite{zhang1995robust}. technique works as follow. At first, for each feature point ${m_i}$ in the first image, it searches for a such ${m_j}$ feature point in the same location of ${m_i}$ but in the second image. The search is relied on the similarity of local image windows centered on the points, which strongly characterizes the points when the images are sufficiently close. The measure for similarity is zero-normalized cross-correlation that is invariant to affine change of the local image intensities and make the procedure robust to illumination changes. To make robust match, the reverse previous procedure is applied from second image to first image. Only the correspondences ${m_i} \longleftrightarrow {m_j}$ between points that close each other are kept.\\
KLT (Kanade-Lucas-Tomasi) tracker \cite{tomasi1991detection} \cite{shi1994good} is another approach to track the feature points across the images. It uses of optical flow to find out the location of feature in following image. Both approaches have their strengths: KLT handles continuity better and keeps tracking points that cannot be detected as interest points. By contrast, performing detection in every frame naturally handles the appearance and disappearance of interest points due to aspect changes and occlusions.

\section {Our Method}
One of the critical part on this master thesis is implementation a robust and accurate feature matching procedure, because the rest phases are completely depend to one. The result of two well-known approaches ,which introduced in previous section, were not perfect that we wanted. Therefore we investigate on a multi layer matcher that remove outliers as well.\\
This novel feature matching consists of 4 consecutive phases that was implemented by OpenCV Ver 3.0 library and Ubitrck Framework. In this recipe, we solve the feature point matching problem and we will explain how to exploit the epipolar constraint between two views to match image features more reliably.\\
The principle we will follow is simple: when we match feature points between two images, we only accept those matches that fall onto the corresponding epipolar lines. However, to be able to check this condition, the fundamental matrix must be known, and we need good matches to estimate this matrix. 
As following, each step describe briefly:
\begin {enumerate}
  \item Brute Force Matching: The first step is simply detecting the feature point and computing their descriptors. Next, we proceed to feature matching using the cv::BruteForceMatcher class as we did in the previous chapter. However, this time we find the two best matching points for each feature (and not only the best one as we did in the previous recipe). This is accomplished by the cv::BruteForceMatcher::knnMatch method (with k=2). Moreover, we perform this matching in two directions, that is, for each point in the first image we find the two best matches in the second image, and then we do the same thing for the feature points of the second image, finding their two best matches in the first image.\\
  The binary descriptors of A-KAZE are matched in a brute force manner with efficient evaluation of the Hamming distance.
  Hamming distance which is computationally cheap on modern architectures with efficient binary XOR and population count instructions.
  \item Nearest Neighbor (Ratio Test) Matching: The second well-known outlier-removal technique is the ratio test. In this phase, for each feature point, we have two candidate matches in the other view. These are the two best ones based on the distance between their descriptors. If this measured distance is very low for the best match, and much larger for the second best match, we can safely accept the first match as a good one since it is unambiguously the best choice. Reciprocally, if the two best matches are relatively close in distance, then there exists a possibility that we make an error if we select one or the other. In this case, we should reject both matches. Here, we perform this test in step 3 by verifying that the ratio of the distance of the best match over the distance of the second best match is not greater than a given threshold. A large number of ambiguous matches will be eliminated by this procedure. After several execution of this method, the best threshold found as 0.8.
  \item Symmetric Matching: We now have two relatively good match sets, one from the first image to second image and the other one from second image to the first one. From these sets, we will now extract the matches that are in agreement with both sets. This is the symmetrical matching scheme imposing that, for a match pair to be accepted, both points must be the best matching feature of the other.
  \item Epipolar Constraint Matching: The last phase now consists of an additional filtering test that will this time use the fundamental matrix in order to reject matches that do not obey the epipolar constraint. This test is based on the RANSAC method that can compute the fundamental matrix even when outliers are still present in the match set. For this one When using the cv::findFundamentalMat function with Ransacs, two extra parameters are provided. The first one is the confidence level that determines the number of iterations to be made. The second one is the maximum distance to the epipolar line for a point to be considered as an inlier. All matched pairs in which a point is at a distance from its epipolar line larger than the one specified will be reported as an outlier. Therefore, the function also returns a std::vector of char value indicating that the corresponding match has been identified as an outlier (0) or as an inlier (1). The inlier points regarding to this mask array and input points from symmetric matching are selected as the final vector in structure of cv::DMatch. This vector illustrate the robust feature match between an image pair.
\end {enumerate}

\section {Method Evaluation}
In previous Section, A new Approach for feature matching was proposed that we call it robust feature matching. It consists of four outliers removal steps to reach a feature matching with high accuracy and reliably. This section provides a performance evaluation of matching precision and speed for each phase of this methods. We evaluate the descriptors on real images with different geometric and photometric transformations and for different scene types. The ground true data set \footnote {\url{http://www.robots.ox.ac.uk/~vgg/research/affine/}} involves six difference type of transformations that illustrates in Fig.
% \autoref{fig:sample-drawing}

\begin{figure}[H]
\centering
\begin{tabular}{cccc}
% \subfloat[A]{\includegraphics[width=65mm]{figures/bike_img3}} & 
% \subfloat[B]{\includegraphics[width=65mm]{figures/bike_img3}} \\ 
  \includegraphics[width=35mm]{figures/bike_img1} &   \includegraphics[width=35mm]{figures/bike_img3}  &
  \includegraphics[width=35mm]{figures/bark_img1} &   \includegraphics[width=35mm]{figures/bark_img3} \\
\multicolumn{2}{c} {(a) blur} &
\multicolumn{2}{c} {(b) zoom + rotation} \\[6pt]
 \includegraphics[width=35mm]{figures/graf_img1} &   \includegraphics[width=35mm]{figures/graf_img3} &
 \includegraphics[width=35mm]{figures/boat_img1} &   \includegraphics[width=35mm]{figures/boat_img3} \\
\multicolumn{2}{c} {(c) viewpoint} &
\multicolumn{2}{c} {(d) zoom + rotation} \\[6pt]
 \includegraphics[width=35mm]{figures/leuven_img1} &   \includegraphics[width=35mm]{figures/leuven_img3} &
 \includegraphics[width=35mm]{figures/ubc_img1} &   \includegraphics[width=35mm]{figures/ubc_img3} \\
\multicolumn{2}{c} {(e) light} &
\multicolumn{2}{c} {(f) JPEG compression} \\[6pt]
\end{tabular}
\caption{caption}
\end{figure}


% See~\autoref{tab:sample}, \autoref{fig:sample-drawing}, \autoref{fig:sample-plot}, \autoref{fig:sample-listing}.

% \begin{figure}[htsb]
%   \centering

%   \pgfplotstableset{col sep=&, row sep=\\}
%   % This should probably go into a file in data/
%   \pgfplotstableread{
%     a & b    \\
%     1 & 1000 \\
%     2 & 1500 \\
%     3 & 1600 \\
%   }\exampleA
%   \pgfplotstableread{
%     a & b    \\
%     1 & 1200 \\
%     2 & 800 \\
%     3 & 1400 \\
%   }\exampleB
%   % This should probably go into a file in figures/
%   \begin{tikzpicture}
%     \begin{axis}[
%         ymin=0,
%         legend style={legend pos=south east},
%         grid,
%         thick,
%         ylabel=Y,
%         xlabel=X
%       ]
%       \addplot table[x=a, y=b]{\exampleA};
%       \addlegendentry{Example A};
%       \addplot table[x=a, y=b]{\exampleB};
%       \addlegendentry{Example B};
%     \end{axis}
%   \end{tikzpicture}
%   \caption[Example plot]{An example for a simple plot.}\label{fig:sample-plot}
% \end{figure}

% \begin{figure}[htsb]
%   \centering
%   \begin{tabular}{c}
%   \begin{lstlisting}[language=SQL]
%     SELECT * FROM tbl WHERE tbl.str = "str"
%   \end{lstlisting}
%   \end{tabular}
%   \caption[Example listing]{An example for a source code listing.}\label{fig:sample-listing}
% \end{figure}
