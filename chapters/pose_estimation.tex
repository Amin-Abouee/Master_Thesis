%!TEX root = main.tex
\chapter{Pose Estimation}\label{chapter:pose_estimation}
\section{Overview}\label{sec:pose_estimation_overview}
As already mentioned in introduction, estimating the pose of camera (also called tracking) is a crucial part in most computer vision applications. In the case of augmented reality applications, currently two approaches have been used: marker-based pose estimation and feature-based pose estimation. In both approaches, the pose estimator tries to make an estimation of camera relative to the scene. Also there are many techniques for both approaches as explained in previous sections. \\
The main point of this thesis is to add one efficient and precise feature-based pose estimation approach to the Ubitrack framework. To reach this goal, some necessary algorithms such as robust feature matching and bundle adjustment were developed and added to Ubitrack. In this section, the mechanism of a novel feature-based pose estimation based on robust feature matching and bundle adjustment will be described in detail. Similar to most new techniques for SLAM, SfM, and tracking topics in computer vision, this task also is divided into two phases: tracking and mapping. The whole procedures for each phase will be explained individually, and at the end the pose estimation based on the tracking and mapping phases will be described. Better understanding of tracking phases, requires some background knowledge that are described in the following sections.

\subsection{Pose estimation by tracking plane}\label{subsec:pose_estimation_planer_tracking}
Pose estimation by tracking plane is one of the well-known methods for pose estimation, based on the feature points proposed by Gilles Simon et.al \cite{genc2002marker}. This method is justified by the fact that there are some rectangular structures such as ground plane, walls, buildings, etc that are visible in the image. At the first, the homography matrix $H_{w}^{0}$ is estimated by several correspondence points which can be given by hand between the first image and 3D world. This matrix is the mapping between the first image (virtual) and the world (real). After that, the method tries to make a homography matrix between the first image and the next one that both of them have the same plane. This pose estimation method is relied on changing of the homographies that can be retrieved by the relation between the plane and a frame of the sequence. This method assumes the homography matrix $H_{t-1}^{t}$ is a map between the frame at time t-1 and the frame at time t that can be computed by the four points matches between them. The final homography that is the relation between the frame at time t and world plane can be estimated by chaining the successive transformation, which can be written as:
\begin{gather*}
	H_{w}^{t} = H_{t-1}^{t} H_{t-2}^{t-1} \cdots H_{0}^{1} H_{w}^{0}
\end{gather*}\label{eq:homography_world_to_refrence}

% TODO: check the homography
% TODO: add image 
In this thesis, we use of this technique to compute an initial pose estimation for each frame of input sequence, where the pose of first camera is taken from the reference system. As the homography matrix uses the SVD method for computing, the result has error due to the noise in input images or for the wide base-line images. To overcome this problem, all homographies are computed in two steps. First, an approximation of homography is estimated by the matching feature points between two images. Then the following image is warped by this homography so that it is roughly aligned with the reference image. Second, the new small baseline homography is computed again but for the warped image and reference image. The multiplication of the first and second homographies computes an accurate and final homography between the images.
\subsection{Computation of homography matrix from the reference projective matrix}
Sometimes, especially for the first frame, the reference homography or $H_{w}^{0}$ that is a mapping between the first image and the world is necessary. Usually, for the beginning of feature tracking, we use a reference system. A projective matrix has been taken by the reference system that should be converted into the correspondence homography matrix. For this purpose, we assume the projective matrix $P$ can project all 3D points on the $z=0$ plane that has the form $X=(x,y,0,1)$ into the first image. So we have:
\begin{align*} 
x  &=  P X \\
   &=  K [r_{1}r_{2}r_{3}t] 
 \begin{pmatrix}
  X \\
  Y \\
  0 \\
  1
 \end{pmatrix} \\
  &=  K [r_{1}r_{2}t] \begin{pmatrix}
  X \\
  Y \\
  1
 \end{pmatrix} \\
  &=  H_{w}^{0} \begin{pmatrix}
  X \\
  Y \\
  1
 \end{pmatrix} \\
\end{align*}
And therefor we have:
\begin{gather*}
	H_{w}^{0} = K [r_{1}r_{2}t]
\end{gather*}\label{eq:homography_to_projective}

\subsection{Implementation of pose estimation based on planner tracking}
% TODO: describe
The data flow of our pose estimation module will be shown in \autoref{fig:pose_estimation}.

\begin{figure}[H]
\begin{tabular}{cc}
  \includegraphics[width=75mm]{figures/pose_estimation} &  \includegraphics[width=75mm]{figures/pose_estimation_body} \\
(a) the body of planer pose estimation  & (b) entry requirements \\[6pt]
\end{tabular}
\caption{The concept of the planer pose estimation}\label{fig:pose_estimation}
\end{figure}

\section{PnP Problem} \label{sec:PnP_problem}
Determining the orientation and translation of a fully calibrated camera with respect to a known 3D model by using n (n>3) 3D points and their image projection is a classical problem in computer vision that is known as the perspective-n-point (PnP) problem. Schweighofer et al. \cite{schweighofer2006robust} represented the PnP problem as a multi model reprojection error function (cost function) with two local minima. One classical solution for this problem is using the Levenbergâ€“Marquardt to minimize this cost function. In general, there are several ways to solve this problem based on the number of the given 3D points. For instance P3P, P4P and P5P. \\
The minimal P3P problem has been properly solved leading to many prominent linear solvers. Generally for PnP problem, currently several excellent noniterative $O(n)$ solutions have been proposed. EPnP \cite{lepetit2009epnp} tries to express all 3D points into the linear combination of four virtual control points, and approximately solve the resulting multivariate polynomial system using linearizion. The other well-known approach in this case is direct linear transformation (DLT) \cite{hartley2003multiple}. First it identifies the projection matrix and then extracts the calibration parameters and the camera pose. \\
Zheng et.al \cite{zheng2013revisiting} listed some PnP solutions that rely on the minimization of reprojection error. For the first time, Uymeyama \cite{umeyama1991least} introduced a linear method for optimization. The work tried to avoid local minimum by relaxing the PnP problem into a semidefinite program. The used memory due to relaxation was not efficient. After that, a direct least square (DLS) method was introduced by Hesch and Roumeliotis \cite{hesch2011direct} with the time complexity $O(n)$. This technique solves the drawback of Uymeyama method by solving the polynomial system.\\
\autoref{fig:pnp_sample} \footnote{\url{http://docs.opencv.org/trunk/doc/tutorials/calib3d/real_time_pose/real_time_pose.html}} shows the concept of PnP problem and the rotation and translation between the scene and the camera.

\begin{figure}[H]
  \centering
  \includegraphics[width=140mm]{figures/pnp}
  \caption{PnP problem: Finding the pose of camera relative to the world coordinate.} \label{fig:pnp_sample}
\end{figure}


\section{Tracking and Mapping}
This section explains the operation of the feature point-based tracking system. The tracking phase receives the input images and estimate the pose of camera relative to the scene. For tracking of each frame, the PnP solution is used to find out the rotation and translation of camera. In the following we will describe our novel approach that groups the input frames into some small packages that is called bundle. The pose of camera is estimated for all frames of the bundle efficiently and robustly. The 3D world model also is computed and updated at the end of the bundle processing.

\subsection{The bundle concept}
As already was mentioned in previous chapters, there are many approaches for feature point-based tracking. PTAM \cite{klein2007parallel} is a well-known approach for tracking the pose in AR workspace. The important point of this technique is that the tracking and mapping are separated and run in two parallel threads. In the case of mapping phase, the obtained 3D world map is optimized by a bundle adjustment after each new frame. The result of bundle adjustment for the market-based tracking in Ubitrack illustrates that the noisy data including the noise in images and also mismatch in matches vector have a negative effect on that.\\
Based on our experience about the result of bundle adjustment for the marker-based tracking (\autoref{subsec:result_ba}), we decided to use the bundle adjustment for a group of images instead of two or three images. For this purpose and after of several tests over the result of bundle adjustment, the input frames are grouped into the same small packages, usually five frames, that is called a bundle. The size of the bundle is a parameters in our approach and we called it local threshold. Based on our evaluation (shown in \autoref{sec:local_threshold}) 5 is the best value for local threshold.\\
The feature points of each frame are extracted by A-KAZE feature detection and extraction. 



After that the robust feature matching is applied to filter out the outliers between the robust feature points of previous frames (***REFRENCE To SECTION***) and new frames in a recursive way. 
The recursive function consists of two steps: forward step from frame 1 to frame 5, and backward step from frame 5 to frame 1. Each step consists of 2D-2D matching of feature points between consequent frames. The result of each matching will be used for matching with the next frame. For instance for the forward step of a bundle with five frame we execute the robust feature matching four times: between the first and the second; the resulting roust feature points with the third frames; the resulting roust feature points with the forth frames; and finally the resulting robust feature points from all previous matchings with the fifth frame (******* FIGURE XXXXXXXXXXXXXXXXX ******). The same procedure is applied in the backward step. 
The result of the recursive robust feature matching function is a vector of robust features for each frame that are also present at all other frames. We call these set of features the \textit{elite feature points}. Note that despite different coordinates in each frame, each elite feature point in each frame has the same feature descriptor (for details about equality of feature descriptors see ******** AUTOR-REF BE OONJAII KE GOFTI CHETOR TO DA FEATURE DESCRIPTOR BA HAM BARABAR HASTAND *******). 

% TODO: Image XXXXXXXXXXXXXXX
 
These elite feature points and the camera pose of all frames are gathered to reconstruct a set of new 3D points of our environment. In Ubitrack, the \detokenize{Algorithm::get3DPositionWithResidual()} function is used for the 3D reconstruction. The size of the resulting 3D points is the same as the size of the elite feature points of each frame. \\

We can also consider 3D points as 3D feature points. For this, for each 3D point the feature descriptor of corresponding elite feature points is allocated to it.  


After the 3D reconstruction, a non-planner pose optimization function \cite{lu2000fast} which is implemented in UBitrack under the name \detokenize{Ubitrack::Algorithm::PoseEstimation2D3D::optimizePose()}. This optimization applies a robust 2D-3D feature matching to the elite feature points, pose of each camera, and the obtained 3D points (\autoref{fig:pose_optimization}).  

\autoref{fig:pose_optimization} demonstrates a schematic view of our pose optimization.
\begin{figure}[H]
  \centering
  \includegraphics[width=100mm]{figures/pose_optimization}
  \caption{The data flow of pose optimization}\label{fig:pose_optimization}
\end{figure}

For the last step of this scenario, a bundle adjustment is used to refine the 3D points. The necessary parameters including the elite feature points of all frames, the optimized camera poses, the estimation of 3D points and the intrinsic of all cameras are passed to the cvsba bundle adjustment with optimization type Sba::STRUCTURE (\autoref{subsec:type_of_optimization}) to optimize the 3D points. The number of iteration is 300 and the minimum of reprojection error is $1\mbox{\textsc{e}}-10$. The final optimized 3D points are added to or updated in the 3D world map vector. This whole process is called a local bundle adjustment, because is applied to only one bundle. \autoref{fig:bundle_concept} illustrates a schematic view (data flow) of our bundle concept.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth, height=\textheight, keepaspectratio]{figures/bundle_concept}
  \caption{The data flow of the bundle concept}\label{fig:bundle_concept}
\end{figure}

\subsection{Pose estimation for the first bundle}\label{subsec:pose_first_bundle}

Based on the new bundle concept that was introduced in previous section, all input frames are grouped into five frames and are processed independent of other bundles. In this section, we describe the operations that are necessary for the first bundle.\\
First of all, we need to determine a prior estimation for the pose of each camera for the 3D reconstruction. Two approaches are considered to compute the initiate pose for each camera inside a bundle:
\begin{enumerate}
\item Initiate pose from the reference system: the pose of all cameras are provided by the reference system.
\item Initiate pose estimation by planner tracking: in this method the pose of the first camera is taken from the pose system and converted to the corresponding homography matrix. The pose of the rest cameras are computed consequently relying on the approach that was explained in \autoref{subsec:pose_estimation_planer_tracking}.
\end{enumerate}
Based on a comparison between these two approaches as shown in \autoref{sec:refrence_system}, the first approach is selected for the initiate estimation of camera poses. These reference poses (initial pose) are used for the 3D reconstruction as described above. The obtained 3D points are in turn used to optimize the camera poses again (precise pose). The resulting poses and 3D points are used for a local bundle adjustment to create the first set of 3D points of the world map. 



\subsection{Pose estimation for the second bundle} \label{subsec:pose_second_bundle}
The scenario for processing the second bundle is a little bit different with the first bundle. In the first bundle, the initial poses have been taken from the reference system whereas the pose estimation of the second bundle's frames are computed by two parts:
\begin{enumerate}
\item The initial pose estimation of each camera are computed by the planner tracking (see \autoref{subsec:pose_estimation_planer_tracking}). For instance the pose of sixth camera (i.e. the first camera of the second bundle) is computed by the homography matrix between the fifth camera and the sixth camera. The pose of other cameras are also determined by this technique sequentially. 
\item For precise pose estimation the elite feature points of the frames of the second bundle are compared with the 3D points in the world map are projected into the image frames according to the initial pose. The result is a 2D-2D matches vector that is used along with the intrinsic parameters of the camera for a non-linear pose optimization (see \autoref{fig:pose_optimization}). 
\end{enumerate}

\autoref{fig:pose_estimation_optimization} demonstrates the two steps of pose estimation. The first step is done by planner tracking technique and the second step by pose optimization.

\begin{figure}[H]
  \centering
  \includegraphics[width=100mm]{figures/pose_estimation_optimization}
  \caption{two steps of pose estimation by using the 3D world map}\label{fig:pose_estimation_optimization}
\end{figure}

The rest operation including: feature extraction, robust feature matching, recursive approach for finding the elite feature points, 3D reconstruction and the local bundle adjustment are exactly the same as the first bundle. The process is repeated for the remaining other bundles. 

\begin{figure}[H]
  \centering
  \includegraphics[angle=90, height=\textheight, keepaspectratio]{figures/bundle_second}
  \caption{The data flow of two connected bundles}\label{fig:bundle_second}
\end{figure}

\subsection{Updating the 3D points (world map)} \label{subsec:update_3d_points}
After processing of each bundle, a set of 3D points are reconstructed. After that, we need to update the 3D world map vector based on these new 3D points. In this thesis, instead of adding the new 3D points to the world map and growing the 3D model of the world, as is usual in other SLAM methods, we only keep the 3D points that are that are already present in the 3D world map. In more details, for the first bundle the optimized 3D points are directly stored into the 3D world map. For the second one, the new optimized 3D points are compared with the 3D world map vector. For this purpose, both 3D sets are projected to the last frame of the last bundle and are compared by a robust feature matching 3D-3D based on their feature descriptors to find the common 3D points. The new 3D world map is updated only by these common 3D points. Hence the size of new 3D world map often is decreased because we only consider the common 3D points and the new points or existing points that do not have a match are ignored. The reason for this approach is only keeping those robust 3D points of the world map that are visible from all frames of all bundles. 
In the next step, a bundle adjustment (global bundle adjustment) is applied to refine these 3D points. For global bundle adjustment we need the 2D elite feature points that reconstructed the new 3D world map. However matching the indices of these 3D points and their corresponding 2D points can be very complicated. To solve this problem, a big data table is used to save the correct index matches between each 3D point and its elite feature points after each local bundle adjustment.\\
Regarding to this fact that the corresponding elite feature points of each 3D point are now known in our data table, the new 3D points are optimized by a cvsba bundle adjustment with the type Sba::STRUCTURE. The \autoref{fig:global_bundle_adjustment} show an overall view of the global bundle adjustment after the processing of the second bundle.

\begin{figure}[H]
  \centering
  \includegraphics[width=120mm]{figures/global_bundle_adjustment}
  \caption{Global bundle adjustment after the second bundle}\label{fig:global_bundle_adjustment}
\end{figure}

As described above, only those 3D points are kept in the world map which robustly matched with the newly computed 3D points. Keeping the robust matched points causes the size of world map decreases after each bundle and after a long time (more than 5 bundles) the remaining 3D points in our world map are not sufficient for mapping phase and pose estimation of the new frames. To prevent this issue, after each seven bundles (35 frames), instead of updating the 3D world map, we just replace the old world map with the 3D points that are directly computed by the local bundle adjustment. For instance, the 3D points of the first to seventh bundle are updated by the global bundle adjustment. But for the eighth bundle, because the size of 3D world map is not big enough (usually less than 100 points), the 3D world map is replaced by the new 3D points after the local bundle adjustment directly. This cycle is repeated after processing of each seventh bundles. \autoref{fig:mapping_concept} illustrates the updating of 3D world map after seven bundles. 

We call the total number of frames to be processed before replacement of the world map, the \textit{global threshold}. The evaluation results are the best for a global threshold of 35 (\autoref{sec:global_threshold}).

\begin{figure}[H]
  \centering
  \includegraphics[width=150mm]{figures/mapping}
  \caption{The value of 3D world map vector after the five bundles}\label{fig:mapping_concept}
\end{figure}

\subsection{Summary} \label{subsec:summary}

In this chapter, we introduced a novel feature-based pose estimation approach. The input frames are grouped into bundles and the 3D points of each bundle are reconstructed based on a four layers robust feature matching and bundle adjustment. To achieve a more reliable pose estimation based on the 3D world map only those 3D points are kept in the world map that are available in all frames of all bundles. On the other side to compensate the reduction of the number of 3D points in the world map, after processing seven bundles the 3D world map is refreshed entirely with the new 3D points resulting from local bundle adjustment. 


In the next chapter we will show the result of this novel approach and compare it with the results of marker-based pose estimation, which was implemented in Ubitarck, on the same scenario and data frames.
