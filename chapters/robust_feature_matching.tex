%!TEX root = main.tex
\chapter{Robust Feature Matching}\label{chapter:Robust Feature Matching}
For pose estimation, we need to find out a match set of feature points ${m_i}$ and ${m_j}$ which are extracted from two successive images with similar view points. The classical technique was proposed by Zhan et al. \cite{zhang1995robust}. This technique works as follow: At first, for each feature point ${m_i}$ in the first image, it searches for a ${m_j}$ feature point in the same location as ${m_i}$ but in the second image. The search is relied on the similarity of local image windows centered on the points, which strongly characterizes the points when the images are sufficiently close. The measure for similarity is zero-normalized cross-correlation that is invariant to affine change of the local image intensities and makes the procedure robust to illumination changes. To make a robust match, the reverse of previous procedure is applied from second image to first image. Only the correspondences ${m_i} \longleftrightarrow {m_j}$ between two points that are close each other are kept.\\
KLT (Kanade-Lucas-Tomasi) tracker \cite{tomasi1991detection} \cite{shi1994good} is another approach to track the feature points across the images. It uses optical flow to find out the location of the features in the following image. Both approaches have their strengths: KLT handles continuity better and keeps tracking points that cannot be detected as feature points. By contrast, performing detection in every frame naturally handles the appearance and disappearance of feature points due to aspect changes and occlusions.

% TODO: add images
% TODO: change the approach. describe th problem
\section {Our Method}
One of the critical parts in this master thesis is the implementation of a robust and accurate feature matching procedure, because the rest parts completely depend on the feature matching . The result of two well-known approaches, introduced in previous section, were not as perfect as we required. Therefore we introduce a novel multilayer matcher that removes outliers substantially.\\
This novel feature matching consists of four consecutive layers. In this approach, we solve the feature point matching problem and we will explain how to exploit the epipolar constraint between two views to match image features more reliably.\\
% TODO: describe briefly the other layers
The principle we will follow is simple: when we match feature points between two images, we only accept those matches that fall onto the corresponding epipolar lines. However, to be able to check this condition, the fundamental matrix must be known, and we need good matches to estimate this matrix. 
In the following, each layer is described briefly:
\begin {enumerate}
  \item Brute Force Matching: In the first layer we apply a brute force matching to the input feature points from two images. For this matching we use \detokenize{cv::BruteForceMatcher} class. In this step for each feature point in the first image the two best matching feature points in the second image are selected by Hamming distance. This process is then reverses from the second image to the first image. This is accomplished by the \detokenize{cv::BruteForceMatcher::knnMatch} method (with k=2).\\
  The binary descriptors of A-KAZE are matched in a brute force manner with efficient evaluation of the Hamming distance which is computationally cheap on modern architectures with efficient binary XOR and population count instructions.
  The result of this layer is two vectors one from first image to the second and vice versa. Each of these two vectors contains a two-member vector for each feature point. That is, the result is a data structure in the form of \detokenize{std::vector < std::vector < cv::CMatch > >}. 
  % TODO: Image
  \item Nearest Neighbor Matching (Ratio Test): The second layer of our outlier-removal technique is the ratio test. In this layer, for each feature point, we have two candidate matches in the other image, resulting from previous layer. These are the two best ones based on the distance between their descriptors. If this measured distance is very low for the best match, and much larger for the second best match, we can safely accept the first match as a good one since it is unambiguously the best choice. On the other hand, if the two best matches are relatively close in distance, then there exists a possibility that we make an error if we select one over the other. In this case, we should reject both matches. Here, we perform this test in layer two by verifying that the ratio of the distance of the best match over the distance of the second best match is not greater than a given threshold. A large number of ambiguous matches will be eliminated by this procedure. After several execution of this method, the best threshold set as 0.8.
  % TODO: add images
  \item Symmetric Matching: We now have two relatively good match sets, resulting from the previous layer: one from the first image to second image and the other one from second image to the first one. From these sets, we will now extract the matches that are in agreement with both sets. This is the symmetrical matching scheme imposing that, for a match pair to be accepted, both points must be the best matching feature of the other. The result of this layer is yet another reduction in the size of matching feature points vector.
  \item Epipolar Constraint Matching: The last layer now consists of an additional filtering test that will this time use the fundamental matrix in order to reject matches that do not obey the epipolar constraint. This test is based on the RANSAC method that can compute the fundamental matrix even when outliers are still present in the match set. For this layer we use the \detokenize{cv::findFundamentalMat} function with Ransacs. This function has two extra parameters. The first one is the confidence level that determines the number of iterations to be made. The second one is the maximum distance to the epipolar line for a point to be considered as an inlier. All matched pairs in which a point is at a distance from its epipolar line larger than the specified threshold will be reported as an outlier. Therefore, the function also returns a \detokenize{std::vector} of char values indicating that the corresponding match has been identified as an outlier (0) or inlier (1). The inlier points regarding to this mask array and input points that come from symmetric matching are selected as the final vector in structure of \detokenize{cv::DMatch}. This vector illustrate the robust feature matches between the image pair.
\end {enumerate}

\section {Method Implementation}
In the previous section, the operation of robust feature matching was described. How it works and how the different layers are connected together to reach the final result. In this section the implementation's procedure and requirements for them are explained. One of the goals on this master thesis is adding the robust feature matching to Ubitrack framework. For this purpose at first we ported the A-KAZE feature descriptor to Ubitrack. The new class is called cvAKAZEFeature. Then, a matcher class for A-KAZE was developed which is a connection between the cvAKAZEFeature and robust feature matching. The name of this class is cvAKAZEFeatureMatcher. In fact, this class take two vectors of A-KAZE features from cvAKAZEFeature and sends them to robust feature matching to find out the final match vector. In the following, the procedure and structure of each of these classes will be described in detail:

\subsection {cvAKAZEFeature}
% TODO: master thesis reference
% TODO: Ubitrack
In Ubitrack, some basic classes that represent the feature descriptor and feature matching, were implemented by Sven Barth in his master thesis with title "Flexible and Extensible Feature Tracking Architecture for the Ubitrack Framework". The basic class of that implementation is an abstract class with name FeatureBase. This class is used for dynamic casting between different kind of feature points that we have.
The next class in this hierarchy that is inherited from FeatureBase is Feature class. The Feature class that consists of a  \detokenize{boost::shared_ptr} of FeatureBase that represents the location of a feature point and a \detokenize{Math::Pose} object that is used for storing the position of the object relative to the camera. The \detokenize{Math::Pose} involves rotation and translation matrices. To cover the OpenCV feature point (implemented by OpenCV), class OpenCVFeature is extended from Feature class. It consists of a \detokenize{cv::keypoint} for storing the data of feature point and \detokenize{cv::Mat} for its feature descriptor vector (binary or float). The cvAKAZEFeature is a sub class of OpenCVFeature that is used for saving the value of AKAZE's feature point and descriptor. The \autoref{fig:cv_akaze} illustrates the hierarchy of cvAKAZEFeature.

\begin{figure}[H]
  \centering
  \includegraphics[width=55mm]{figures/cv_akaze}
  \caption{The hierarchy of cvAkazeFeature class in Ubitrack Framework}\label{fig:cv_akaze}
\end{figure}
 
% \subsection {cvAKAZEFeatureMatcher}
\subsection {RobustFeatureMatching} \label{subsec:robust_feature_matching}
This part is about the implementation of our feature matching in Ubitrack Framework. Firstly, one abstract class with name RobustFeatureMatching is implemented. This class is cornerstone of our idea. As it was mentioned in previous section, two types of descriptors exist. The binary descriptor and the float descriptor. Two classes RobustFetureMatchingBitVecFeatureBase and RobustFetureMatchingFloatFeatureBase were extended to support both types of descriptors. Both of them contains four functions for processing the brute force, nearest neighbor, symmetric and epipolar constraint matching layers. The output of each function is the input of next function and all functions call consecutively.The \autoref{fig:cv_akaze_matching} show the structure of RobustFeatureMatching in Ubitrack Framework.

\begin{figure}[H]
  \centering
  \includegraphics[width=140mm]{figures/robust_feature_matching}
  \caption{The hierarchy of RobustFeatureMatching class in Ubitrack Framework}\label{fig:cv_akaze_matching}
\end{figure}


\section {Method Evaluation}
In previous section, a new approach for feature matching was proposed that we call it robust feature matching. It consists of four outlier removal layers to reach a feature matching with high accuracy and reliability. This section provides a performance evaluation of matching precision and speed for each layer of this method. We evaluate the matching on real images with different geometric and photometric transformations and for different scene types. The ground truth data set \footnote {\url{http://www.robots.ox.ac.uk/~vgg/research/affine/}} involves six difference types of transformations that are illustrated in \autoref{fig:matching_dataset_oxford}.

% TODO: add name to figure
\begin{figure}[H]
\centering
\begin{tabular}{cccc}
  \includegraphics[width=35mm]{figures/bike_img1} &   \includegraphics[width=35mm]{figures/bike_img3}  &
  \includegraphics[width=35mm]{figures/bark_img1} &   \includegraphics[width=35mm]{figures/bark_img3} \\
  \multicolumn{2}{c} {(a) blur} &
  \multicolumn{2}{c} {(b) zoom + rotation} \\[6pt]
  \includegraphics[width=35mm]{figures/graf_img1} &   \includegraphics[width=35mm]{figures/graf_img3} &
  \includegraphics[width=35mm]{figures/boat_img1} &   \includegraphics[width=35mm]{figures/boat_img3} \\
  \multicolumn{2}{c} {(c) viewpoint} &
  \multicolumn{2}{c} {(d) zoom + rotation} \\[6pt]
  \includegraphics[width=35mm]{figures/leuven_img1} &   \includegraphics[width=35mm]{figures/leuven_img3} &
  \includegraphics[width=35mm]{figures/ubc_img1} &   \includegraphics[width=35mm]{figures/ubc_img3} \\
  \multicolumn{2}{c} {(e) light} &
  \multicolumn{2}{c} {(f) JPEG compression} \\[6pt]
\end{tabular}
\caption{Data set. Examples of images used for the evaluation: (a) blur, (b) zoom + rotation, (c) viewpoint change, (d) zoom + rotation, (e) light change and (f) JPEG compression.}\label{fig:matching_dataset_oxford}
\end{figure}

Based on work of MIK + >>> \cite{mikolajczyk2005comparison}, the viewpoint change test varies from a fronto-parallel view to one with significant foreshortening at approximately 60 degrees to the camera. The scale change and blur sequences are acquired by varying the camera zoom and focus respectively. The scale changes by about a factor of four. The light changes are introduced by varying the camera aperture. The JPEG sequence is generated using a standard xv image browser with the image quality parameter varying from 40\% to 2\%. Each of the test sequences contains 6 images with a gradual geometric or photometric transformation. All images are of medium resolution (approximately 800 x 640 pixels). The images are either of planar scenes or the camera position is fixed during acquisition, so that in all cases the images are related by homographies (plane projective transformations). This means that the mapping between each pair of images is known (or can be computed). This mapping is used to determine ground truth matches. All the images as well as the computed homographies are available on the website.\\
For performance evaluation, there are some measurement techniques (\cite{mikolajczyk2005comparison} \cite{mikolajczyk2005performance}) such as repeatability score, precision-recall, speed-up and time that here, the repeatability score and computational time were considered as evaluation criteria.\\
% TODO: split into 2 sentences
Generally to evaluate the obtained feature matcher, we calculated the euclidean distance between the detected feature point in the reference image and feature point detected in the next image via the feature matcher and the projected feature point from reference image by using of ground truth homography that connect the reference image to following image. \autoref{fig:euclidean_distance_error} shows the euclidean distance between the projected feature point and estimated feature point.
\begin{figure}[H]
  \centering
  \includegraphics[width=100mm]{figures/euclidean_distance}
  \caption{The euclidean distance between the projected feature point by ground truth homography and estimated feature point by feature matching. }\label{fig:euclidean_distance_error}
\end{figure}

In this thesis two important parameters characterize the performance of robust feature matching:
\begin{itemize}
  \item Repeatability: is the ratio number of correct matches in images under different geometric and photometric transformations. We say two feature points are matched if the euclidean distance between the projected feature point and estimated feature point are less that 2 pixel.\\
  $$repeatability\_score = \frac{\# correct matches}{\# correspondences}$$
  \item Time: is the computational cost for execution the robust feature matching. All the measurements have been taken on a 2.4 GHz Intel core i5, 4 GB RAM, Mac osx 10.9.5, the code also is compiled by the Clang version 6.0 (600.0.56).
\end{itemize}

In the following, the performance of each layer of our feature matching in the case of repeatability score and computational time are estimated. We show how many of the outliers are filtered and how much the performance of matching is enhanced after processing of each layer. For this evaluation, six different types of image transformations (\autoref{fig:matching_dataset_oxford}) are analyzed. The reference image is always the image of highest quality and is located as the leftmost image of each data set with index 0. The following image also is the third image of each data set.

\subsection {Brute Force Matching}
\begin{figure}[H]
\begin{tabular}{cc}
  \includegraphics[width=75mm]{figures/bike_brute_1_3} &  \includegraphics[width=75mm]{figures/barks_brute_1_3} \\
(a) Bike(blur) & (b) Bark(zoom + rotation) \\[6pt]
 \includegraphics[width=75mm]{figures/graffiti_brute_1_3} &  \includegraphics[width=75mm]{figures/boat_brute_1_3} \\
(c) Graffiti(viewpoint) & (d) Boat(zoom + rotation) \\[6pt]
 \includegraphics[width=75mm]{figures/leuven_brute_1_3} &  \includegraphics[width=75mm]{figures/ubc_brute_1_3} \\
(e) Leuven(light) & (f) UBC(JPEG compression) \\[6pt]
\end{tabular}
\caption{The brute-force matching layer for different image transformations}\label{fig:brute_force_matching}
\end{figure}

\begin{table}[H]
  \begin{tabular}{| c || c | c | c | c | c | c | c |}
      \hline
      data set & Bike & Bark & Graffiti & Boat & Leuven & UBC \\ \hline \hline
      correspondences & 1825 & 1053 & 2947 & 5891 & 1843 & 2937 \\ \hline
      correct matches & 1123 & 108 & 510 & 1456 & 833 & 2576 \\ \hline
      repeatability score & 0.615342 & 0.102564 & 0.173057 & 0.247157 & 0.45198 & 0.877085\\ \hline
      time (ms) & 151.445 & 65.602 & 637.152 & 1716.15 & 122.205 & 638.884 \\ \hline
  \end{tabular}
  \caption{brute-force performance evaluation} \label{tab:brute_force_matching_eval}
\end{table}

As you can see in \autoref{fig:brute_force_matching} and based on \autoref{tab:brute_force_matching_eval}, except for Bike and UBC date sets, the result of repeatability score for the rest data sets are significantly low. For instance, the repeatability for graffiti and bark data sets are around the 0.15 and also so many mismatch features are shown in \autoref{fig:brute_force_matching} (b) and \autoref{fig:brute_force_matching} (c). In the case of computational cost, the maximum amount is allocated to Boat data set because it has the highest number of feature points in this test. The time row of \autoref{tab:brute_force_matching_eval} illustrates that the computations time is proportional to the number of feature points that were detected by A-KAZE.

\subsection {Nearest Neighbor Matching (Ratio Test)}
\begin{figure}[H]
\begin{tabular}{cc}
  \includegraphics[width=75mm]{figures/bike_nn_1_3} &  \includegraphics[width=75mm]{figures/barks_nn_1_3} \\
(a) Bike(blur) & (b) Bark(zoom + rotation) \\[6pt]
 \includegraphics[width=75mm]{figures/graffiti_nn_1_3} &  \includegraphics[width=75mm]{figures/boat_nn_1_3} \\
(c) Graffiti(viewpoint) & (d) Boat(zoom + rotation) \\[6pt]
 \includegraphics[width=75mm]{figures/leuven_nn_1_3} &  \includegraphics[width=75mm]{figures/ubc_nn_1_3} \\
(e) Leuven(light) & (f) UBC(JPEG compression) \\[6pt]
\end{tabular}
\caption{The nearest neighbor matching layer for different of image transformations}\label{fig:nearest_neighbor_matching}
\end{figure}
% TO: cumulative time

\begin{table}[H]
  \begin{tabular}{| c || c | c | c | c | c | c | c |}
      \hline
      data set & Bike & Bark & Graffiti & Boat & Leuven & UBC \\ \hline \hline
      correspondences & 1181 & 147 & 369 & 1648 & 952 & 2548 \\ \hline
      correct matches & 1069 & 83 & 191 & 1183 & 791 & 2497 \\ \hline
      repeatability score & 0.905165 & 0.564626 & 0.517615 & 0.71784 & 0.830882 & 0.979984\\ \hline
      time (ms) & 151.744 & 65.276 & 629.011 & 1682.89 & 121.847 & 527.787 \\ \hline
  \end{tabular}
  \caption{nearest neighbor performance evaluation} \label{tab:nearest_neighbor_matching_eval}
\end{table}

The repeatability score of \autoref{tab:nearest_neighbor_matching_eval} and \autoref{tab:brute_force_matching_eval} show the positive effect of nearest neighbor layer which is applied to the output matches of previous brute force layer. For all cases, repeatability score significantly increases and in some cases it is doubled. For example the repeatability of Bark data set reached to 0.56 whereas it was just 0.10 after the brute force matching. Furthermore, the repeatability for both Bike and UBC data sets have become more than 0.90. The time for all cases are similar to brute force matching. It means the nearest neighbor is a lightweight procedure and generally most of the time is spent at the brute force matching layer.

\subsection {Symmetric Matching}
\begin{figure}[H]
\begin{tabular}{cc}
  \includegraphics[width=75mm]{figures/bike_sym_1_3} &  \includegraphics[width=75mm]{figures/barks_sym_1_3} \\
(a) Bike(blur) & (b) Bark(zoom + rotation) \\[6pt]
 \includegraphics[width=75mm]{figures/graffiti_sym_1_3} &  \includegraphics[width=75mm]{figures/boat_sym_1_3} \\
(c) Graffiti(viewpoint) & (d) Boat(zoom + rotation) \\[6pt]
 \includegraphics[width=75mm]{figures/leuven_sym_1_3} &  \includegraphics[width=75mm]{figures/ubc_sym_1_3} \\
(e) Leuven(light) & (f) UBC(JPEG compression) \\[6pt]
\end{tabular}
\caption{The symmetric matching for different of image transformation}\label{fig:symmetric_matching}
\end{figure}

\begin{table}[H]
  \begin{tabular}{| c || c | c | c | c | c | c | c |}
      \hline
      data set & Bike & Bark & Graffiti & Boat & Leuven & UBC \\ \hline \hline
      correspondences & 1054 & 87 & 190 & 1316 & 785 & 2485 \\ \hline
      correct matches & 1034 & 68 & 114 & 1101 & 745 & 2470 \\ \hline
      repeatability score & 0.981025 & 0.781609 & 0.6 & 0.836626 & 0.949045 & 0.993964 \\ \hline
      time (ms) & 151.983 & 64.549 & 635.564 & 1684.69 & 122.429 & 538.028 \\ \hline
  \end{tabular}
  \caption{Symmetric performance evaluation} \label{tab:symmetric_matching_eval}
\end{table}

After applying the third layer, the symmetric matching layer, approximately the wrong matching lines fade off from \autoref{fig:symmetric_matching}. The \autoref{tab:symmetric_matching_eval} also shows the repeatability score for Bike, Boat, Leuven and UBC reach values more than 0.85.

\subsection {Epiplar-Constraint Matching}
\begin{figure}[H]
\begin{tabular}{cc}
  \includegraphics[width=75mm]{figures/bike_final_1_3} &  \includegraphics[width=75mm]{figures/barks_final_1_3} \\
(a) Bike(blur) & (b) Bark(zoom + rotation) \\[6pt]
 \includegraphics[width=75mm]{figures/graffiti_final_1_3} &  \includegraphics[width=75mm]{figures/boat_final_1_3} \\
(c) Graffiti(viewpoint) & (d) Boat(zoom + rotation) \\[6pt]
 \includegraphics[width=75mm]{figures/leuven_final_1_3} &  \includegraphics[width=75mm]{figures/ubc_final_1_3} \\
(e) Leuven(light) & (f) UBC(JPEG compression) \\[6pt]
\end{tabular}
\caption{The epiplar-constraint matching for different of image transformations}\label{fig:epipolar_matching}
\end{figure}

\begin{table}[H]
  \begin{tabular}{| c || c | c | c | c | c | c | c |}
      \hline
      data set & Bike & Bark & Graffiti & Boat & Leuven & UBC \\ \hline \hline
      correspondences & 1008 & 64 & 102 & 869 & 695 & 2457 \\ \hline
      correct matches & 1005 & 59 & 83 & 842 & 686 & 2456 \\ \hline
      repeatability score & 0.997024 & 0.921875 & 0.813725 & 0.96893 & 0.98705 & 0.999593 \\ \hline
      time (ms) & 152.601 & 67.302 & 641.804 & 1711.59 & 191.496 & 614.149 \\ \hline
  \end{tabular}
  \caption{Epiplar-Constraint performance evaluation} \label{tab:epipolar_matching_eval}
\end{table}

The \autoref{fig:epipolar_matching} is the target result in our plan. The number of mismatch features reach to minimum. For instance just one error match and a repeatability score near the 100\% of accuracy obtained for UBC sample. Approximately in all cases the accuracy measure are more than 90\%. A comparison between the accuracy of epipolar show it gets 3 times better than the brute force match for data set >>>>. In the case of time also we have an overhead of 3 or 4 ms for epipolar matching layer.\\
As we mentioned in all above tests, we just included the first image with the best quality and the third image of each data set. In the next test, we run the robust feature matching over for all images in Wall and Trees that are sample of viewpoint and blur changes respectively. 

\begin{figure}[H]
\begin{tabular}{cc}
  \includegraphics[width=75mm]{figures/wall_final_1_2} &  \includegraphics[width=75mm]{figures/wall_final_1_3} \\
(a) wall (blur) & (b) wall (blur) \\[6pt]
  \includegraphics[width=75mm]{figures/wall_final_1_4} &  \includegraphics[width=75mm]{figures/wall_final_1_5} \\
(c) wall (blur) & (d) wall (blur) \\[6pt]
  \multicolumn{2}{c}{\includegraphics[width=75mm]{figures/wall_final_1_6}} \\
  \multicolumn{2}{c}{wall (blur)} \\[6pt]
\end{tabular}
\caption{The wall data set for viewpoint transformation}\label{fig:epipolar_matching_wall}
\end{figure}

\begin{table}[H]
  \begin{tabular}{| c || c | c | c | c | c | c | c |}
      \hline
      Wall data set & image 1 \& 2 & image 1 \& 3 & image 1 \& 4 & image 1 \& 5 & image 1 \& 6 \\ \hline \hline
      correspondences & 1870 & 899 & 283 & 41 & 8 \\ \hline
      correct matches & 1756 & 896 & 208 & 23 & 1 \\ \hline
      repeatability score & 0.939037 & 0.996663 & 0.734982 & 0.560976 & 0.125 \\ \hline
      time (ms) & 1347.58 & 1287.7 & 1373.6 & 1275.73 & 1302.77 \\ \hline
  \end{tabular}
  \caption{Robust feature matching performance evaluation for all images in Wall data set} \label{tab:epipolar_matching_wall_eval}
\end{table}

According to \autoref{fig:epipolar_matching_wall} and \autoref{tab:epipolar_matching_wall_eval}, the repeatability score between the first, second and third pair are around the 0.99 but for the rest pairs, This score decreases from 0.73 to 0.125. The result of last pair illustrates that robust feature matching can not be applied on the images with wide viewpoint changes (wide-base-line). This test also shows that the number of correspondences points and correct matches for the last pair (1 and 6) are 8 and 1 respectively that is not sufficient for our approach.

\begin{figure}[H]  
\begin{tabular}{cc}
  \includegraphics[width=75mm]{figures/tree_final_1_2} &  \includegraphics[width=75mm]{figures/tree_final_1_3} \\
(a) Tree (blur) & (b) Tree (blur) \\[6pt]
  \includegraphics[width=75mm]{figures/tree_final_1_4} &  \includegraphics[width=75mm]{figures/tree_final_1_5} \\
(c) Tree (blur) & (d) Tree (blur) \\[6pt]
  \multicolumn{2}{c}{\includegraphics[width=75mm]{figures/tree_final_1_6}} \\
  \multicolumn{2}{c}{Tree (blur) }\\[6pt]
\end{tabular}
\caption{The Tree data set for viewpoint transformation}\label{fig:epipolar_matching_tree}
\end{figure}

\begin{table}[H]
  \begin{tabular}{| c || c | c | c | c | c | c | c |}
      \hline
      Tree data set & imge 1 \& 2 & image 1 \& 3 & image 1 \& 4 & image 1 \& 5 & image 1 \& 6 \\ \hline \hline
      correspondences & 1773 & 1247 & 734 & 392 & 193 \\ \hline
      correct matches & 1663 & 968 & 456 & 271 & 110 \\ \hline
      repeatability score & 0.937958 & 0.776263 & 0.621253 & 0.691327 & 0.569948 \\ \hline
      time (ms) & 3938.41 & 4024.37 & 3426.66 & 2544.6 & 1850.59 \\ \hline
  \end{tabular}
  \caption{Epiplar-Constraint performance evaluation} \label{tab:epipolar_matching_tree_eval}
\end{table}

The obtained result for Trees data set are similar to Wall data set. The repeatability score drops from first pair to the last pair because the difference between the images become more and consequently, detecting the correct matches between features gets more difficult. The correct matches for the last pair is 110 whereas the correct matches for the same pair of Wall was 1 feature. It shows that the wide viewpoint change is more complicated than the blur case. To solve this problem, it is better to take the images with shorter viewpoint change. 

\subsection {Overall View}
\begin{table}[H]
\centering
  \begin{tabular}{| c || c | c | c | c | c |}
      \hline
      Data Set & Brute Force & Nearest Neighbor & Symmetric & Epipolar Constraint \\ \hline \hline
      Mean & 0.411198 & 0.752685 & 0.857045 & 0.948033 \\  \hline
      Standard Deviation & 0.270575 & 0.169739 & 0.1383 & 0.065499 \\  \hline
  \end{tabular}
  \caption{The mean and standard deviation of repeatability score} \label{tab:repeatability_matching_statistic}
\end{table}

The \autoref{tab:repeatability_matching_statistic} is a statistical analysis of each layer of robust feature matching. It shows the average and standard devision of repeatability score of all data sets that are described in \autoref{fig:matching_dataset_oxford}. The mean of repeatability score has a growth path from 0.411198 to 0.948033 and the repeatability score increases in each layer. Furthermore, the standard devision, which represents the error around the average, decreases from 0.27 to 0.06. So both mean and standard devision show that the accuracy of each layer improves sequentially. This provides an evidence of efficiency of our approach.